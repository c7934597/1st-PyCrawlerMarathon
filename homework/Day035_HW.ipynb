{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加速：多線程爬蟲\n",
    "\n",
    "\n",
    "\n",
    "* 了解知乎 API 使用方式與回傳內容\n",
    "* 撰寫程式存取 API 且添加標頭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "* 找一個之前實作過的爬蟲改用多線程改寫，比較前後時間的差異。\n",
    "\n",
    "單線程用時:13秒\n",
    "\n",
    "多線程用時：1秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 單線程爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [問卦] 聽說去斯洛伐克有很多妹妹 - https://www.ptt.cc/bbs/Gossiping/M.1579147723.A.19E.html\n",
      "Parse Re: [新聞] 「卡神」楊蕙如辱外交官案移審 北院完成分案 - https://www.ptt.cc/bbs/Gossiping/M.1579147761.A.EC7.html\n",
      "Parse [問卦] 涮乃葉的本體是咖哩醬ㄇ？ - https://www.ptt.cc/bbs/Gossiping/M.1579147845.A.9FA.html\n",
      "Parse [新聞] 賴品妤高中制服嫩照流出！網暴動：超像日本女星 - https://www.ptt.cc/bbs/Gossiping/M.1579147865.A.A66.html\n",
      "Parse [問卦] 有沒有台灣內社的卦？ - https://www.ptt.cc/bbs/Gossiping/M.1579147873.A.268.html\n",
      "Parse [問卦] 老鷹 - https://www.ptt.cc/bbs/Gossiping/M.1579147873.A.6E6.html\n",
      "Parse Re: [問卦] 抓小雞~(@[email protected])~排妹到底憑什麼可以一直亂嗆?~ - https://www.ptt.cc/bbs/Gossiping/M.1579147891.A.F51.html\n",
      "Parse Re: [問卦]聽說去斯/|||\\ 洛伐克有很多妹妹 - https://www.ptt.cc/bbs/Gossiping/M.1579147892.A.58C.html\n",
      "Parse Re: [新聞] 日媒報導從總統選舉看見日本影響力衰退 - https://www.ptt.cc/bbs/Gossiping/M.1579147919.A.5D4.html\n",
      "Parse [問卦] 美國才是最會畫三角的國家嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1579147949.A.ACA.html\n",
      "Parse [問卦] 美國要台跟中買他的豬牛意義？ - https://www.ptt.cc/bbs/Gossiping/M.1579148033.A.80C.html\n",
      "Parse Re: [問卦] 選完才三天 館長就被背叛了？ - https://www.ptt.cc/bbs/Gossiping/M.1579148040.A.6DE.html\n",
      "Parse [問卦] 有沒有亞太電信的八卦？ - https://www.ptt.cc/bbs/Gossiping/M.1579148057.A.4A5.html\n",
      "Parse [問卦] 小吃攤很少提供番茄醬？？？ - https://www.ptt.cc/bbs/Gossiping/M.1579148085.A.92B.html\n",
      "Parse [問卦] 妹子在外穿拖鞋怎麼了嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1579148113.A.B7B.html\n",
      "Parse Re: [新聞] 日媒報導從總統選舉看見日本影響力衰退 - https://www.ptt.cc/bbs/Gossiping/M.1579148116.A.2BA.html\n",
      "Parse [問卦] 絕對可憐少女有多可憐？ - https://www.ptt.cc/bbs/Gossiping/M.1579148152.A.43D.html\n",
      "Parse [問卦] 在PTT或電視媒體上罵人哪個比較多人看到 - https://www.ptt.cc/bbs/Gossiping/M.1579148167.A.F3F.html\n",
      "Parse [新聞] 台南維冠大樓5黑心建商判賠7.1億　偷工 - https://www.ptt.cc/bbs/Gossiping/M.1579148219.A.8FC.html\n",
      "Parse Re: [新聞] 「數學不到7級分就去學焊接」南韓 - https://www.ptt.cc/bbs/Gossiping/M.1579148329.A.C77.html\n",
      "Reach the last article\n",
      "共用時： 13.089041709899902\n"
     ]
    }
   ],
   "source": [
    "def crawl_article(url):\n",
    "    response = requests.get(url, cookies={'over18': '1'})\n",
    "    \n",
    "    # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "    if response.status_code != 200:\n",
    "        print('Error - {} is not available to access'.format(url))\n",
    "        return\n",
    "    \n",
    "    # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # 取得文章內容主體\n",
    "    main_content = soup.find(id='main-content')\n",
    "    \n",
    "    # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "    metas = main_content.select('div.article-metaline') #list\n",
    "    author = ''\n",
    "    title = ''\n",
    "    date = ''\n",
    "    if metas:\n",
    "        if metas[0].select('span.article-meta-value')[0]:\n",
    "            author = metas[0].select('span.article-meta-value')[0].string\n",
    "        if metas[1].select('span.article-meta-value')[0]:\n",
    "            title = metas[1].select('span.article-meta-value')[0].string\n",
    "        if metas[2].select('span.article-meta-value')[0]:\n",
    "            date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "        # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "        #\n",
    "        # .extract() 方法可以參考官方文件\n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "        for m in metas:\n",
    "            m.extract()\n",
    "        for m in main_content.select('div.article-metaline-right'):\n",
    "            m.extract()\n",
    "    \n",
    "    # 取得留言區主體\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for p in pushes:\n",
    "        p.extract()\n",
    "    \n",
    "    # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "    # 透過 regular expression 取得 IP\n",
    "    # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "    try:\n",
    "        ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "        ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "    except Exception as e:\n",
    "        ip = ''\n",
    "    \n",
    "    # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    #\n",
    "    # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "    #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "    filtered = []\n",
    "    for v in main_content.stripped_strings:\n",
    "        # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "        if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "            filtered.append(v)\n",
    "\n",
    "    # 定義一些特殊符號與全形符號的過濾器\n",
    "    expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "    \n",
    "    # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "    filtered = [i for i in filtered if i]\n",
    "    content = ' '.join(filtered)\n",
    "    \n",
    "    # 處理留言區\n",
    "    # p 計算推文數量\n",
    "    # b 計算噓文數量\n",
    "    # n 計算箭頭數量\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    for push in pushes:\n",
    "        # 假如留言段落沒有 push-tag 就跳過\n",
    "        if not push.find('span', 'push-tag'):\n",
    "            continue\n",
    "        \n",
    "        # 過濾額外空白與換行符號\n",
    "        # push_tag 判斷是推文, 箭頭還是噓文\n",
    "        # push_userid 判斷留言的人是誰\n",
    "        # push_content 判斷留言內容\n",
    "        # push_ipdatetime 判斷留言日期時間\n",
    "        push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "        push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "        push_content = push.find('span', 'push-content').strings\n",
    "        push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "        push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 整理打包留言的資訊, 並統計推噓文數量\n",
    "        messages.append({\n",
    "            'push_tag': push_tag,\n",
    "            'push_userid': push_userid,\n",
    "            'push_content': push_content,\n",
    "            'push_ipdatetime': push_ipdatetime})\n",
    "        if push_tag == u'推':\n",
    "            p += 1\n",
    "        elif push_tag == u'噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "    \n",
    "    # 統計推噓文\n",
    "    # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "    # all 為總共留言數量 \n",
    "    message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "    \n",
    "    # 整理文章資訊\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'article_author': author,\n",
    "        'article_title': title,\n",
    "        'article_date': date,\n",
    "        'article_content': content,\n",
    "        'ip': ip,\n",
    "        'message_count': message_count,\n",
    "        'messages': messages\n",
    "    }\n",
    "    return data\n",
    "\n",
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 呼叫上面寫好的 function 來對文章進行爬蟲\n",
    "        if article_URL:\n",
    "            parse_data = crawl_article(article_URL) # 返回單一文章資訊的字典\n",
    "        \n",
    "        # 將爬完的資料儲存\n",
    "        all_data.append(parse_data)\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多線程爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [新聞] 找黃國昌當民眾黨副主席？ 柯文哲：可以 - https://www.ptt.cc/bbs/Gossiping/M.1579140639.A.D2F.html\n",
      "Parse [新聞] 李彥秀為何輸高嘉瑜？ 沈：是被韓拖下來 - https://www.ptt.cc/bbs/Gossiping/M.1579140651.A.6E3.html\n",
      "Parse [問卦] 真禮姐姐愛喝酒該怎麼辦 - https://www.ptt.cc/bbs/Gossiping/M.1579140652.A.4A6.html\n",
      "Parse Re: [新聞] 「民進黨不會全力罷韓」　柯文哲：留下韓 - https://www.ptt.cc/bbs/Gossiping/M.1579140699.A.D4C.html\n",
      "Parse [新聞] 2020高雄愛河燈會布置曝光！網友：拳頭硬 - https://www.ptt.cc/bbs/Gossiping/M.1579140705.A.D11.html\n",
      "Parse [問卦] 進餐廳前要喊什麼 - https://www.ptt.cc/bbs/Gossiping/M.1579140735.A.E65.html\n",
      "Parse Re: [問卦] 中美第一階段貿易協議 - https://www.ptt.cc/bbs/Gossiping/M.1579140775.A.D16.html\n",
      "Parse [新聞] 作文題目「我的繽紛世界」9歲女孩寫不出 - https://www.ptt.cc/bbs/Gossiping/M.1579140785.A.040.html\n",
      "Parse [新聞] 60歲加藤鷹曝近照　「滿臉皺紋+深眼袋」 - https://www.ptt.cc/bbs/Gossiping/M.1579140790.A.5E0.html\n",
      "Parse [新聞] 影／辦演唱會被抗議？　高嘉瑜：還有人 - https://www.ptt.cc/bbs/Gossiping/M.1579140807.A.2FF.html\n",
      "Parse Re: [新聞] 芬蘭第一大報關注資源回收 譽台灣為全球 - https://www.ptt.cc/bbs/Gossiping/M.1579140857.A.2B0.html\n",
      "Parse [問卦] 獵人畫到哪了 - https://www.ptt.cc/bbs/Gossiping/M.1579141045.A.9AD.html\n",
      "Parse [問卦] 匪區的疫情不斷，是人禍還是天災? - https://www.ptt.cc/bbs/Gossiping/M.1579141137.A.3A0.html\n",
      "Parse [問卦] HTC會重返榮耀嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1579141144.A.681.html\n",
      "Parse [問卦] 過年要包多少錢紅包給小孩的八卦? - https://www.ptt.cc/bbs/Gossiping/M.1579141176.A.3B6.html\n",
      "Parse [新聞] 韓市政一年空轉 高雄人坦白:不得已才投蔡 - https://www.ptt.cc/bbs/Gossiping/M.1579141530.A.CE6.html\n",
      "Parse [新聞] 入伍生操500公尺障礙訓練熱死　家屬獲國 - https://www.ptt.cc/bbs/Gossiping/M.1579141577.A.02F.html\n",
      "Parse [問卦] 今年沒刮刮樂之亂？ - https://www.ptt.cc/bbs/Gossiping/M.1579141594.A.E5C.html\n",
      "Parse Re: [新聞] 國民黨敗選檢討報告 李正皓看完氣炸：你 - https://www.ptt.cc/bbs/Gossiping/M.1579141595.A.393.html\n",
      "Parse Re: [新聞] 「民進黨大勝」背後警訊！不分區得票率竟 - https://www.ptt.cc/bbs/Gossiping/M.1579141707.A.D2B.html\n",
      "Reach the last article\n",
      "共用時： 1.1396284103393555\n"
     ]
    }
   ],
   "source": [
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "def crawl_article(url):\n",
    "    response = requests.get(url, cookies={'over18': '1'})\n",
    "    \n",
    "    # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "    if response.status_code != 200:\n",
    "        print('Error - {} is not available to access'.format(url))\n",
    "        return\n",
    "    \n",
    "    # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # 取得文章內容主體\n",
    "    main_content = soup.find(id='main-content')\n",
    "    \n",
    "    # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "    metas = main_content.select('div.article-metaline') #list\n",
    "    author = ''\n",
    "    title = ''\n",
    "    date = ''\n",
    "    if metas:\n",
    "        if metas[0].select('span.article-meta-value')[0]:\n",
    "            author = metas[0].select('span.article-meta-value')[0].string\n",
    "        if metas[1].select('span.article-meta-value')[0]:\n",
    "            title = metas[1].select('span.article-meta-value')[0].string\n",
    "        if metas[2].select('span.article-meta-value')[0]:\n",
    "            date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "        # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "        #\n",
    "        # .extract() 方法可以參考官方文件\n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "        for m in metas:\n",
    "            m.extract()\n",
    "        for m in main_content.select('div.article-metaline-right'):\n",
    "            m.extract()\n",
    "    \n",
    "    # 取得留言區主體\n",
    "    pushes = main_content.find_all('div', class_='push')\n",
    "    for p in pushes:\n",
    "        p.extract()\n",
    "    \n",
    "    # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "    # 透過 regular expression 取得 IP\n",
    "    # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "    try:\n",
    "        ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "        ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "    except Exception as e:\n",
    "        ip = ''\n",
    "    \n",
    "    # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "    # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "    #\n",
    "    # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "    #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "    filtered = []\n",
    "    for v in main_content.stripped_strings:\n",
    "        # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "        if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "            filtered.append(v)\n",
    "\n",
    "    # 定義一些特殊符號與全形符號的過濾器\n",
    "    expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "    for i in range(len(filtered)):\n",
    "        filtered[i] = re.sub(expr, '', filtered[i])\n",
    "    \n",
    "    # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "    filtered = [i for i in filtered if i]\n",
    "    content = ' '.join(filtered)\n",
    "    \n",
    "    # 處理留言區\n",
    "    # p 計算推文數量\n",
    "    # b 計算噓文數量\n",
    "    # n 計算箭頭數量\n",
    "    p, b, n = 0, 0, 0\n",
    "    messages = []\n",
    "    for push in pushes:\n",
    "        # 假如留言段落沒有 push-tag 就跳過\n",
    "        if not push.find('span', 'push-tag'):\n",
    "            continue\n",
    "        \n",
    "        # 過濾額外空白與換行符號\n",
    "        # push_tag 判斷是推文, 箭頭還是噓文\n",
    "        # push_userid 判斷留言的人是誰\n",
    "        # push_content 判斷留言內容\n",
    "        # push_ipdatetime 判斷留言日期時間\n",
    "        push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "        push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "        push_content = push.find('span', 'push-content').strings\n",
    "        push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "        push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 整理打包留言的資訊, 並統計推噓文數量\n",
    "        messages.append({\n",
    "            'push_tag': push_tag,\n",
    "            'push_userid': push_userid,\n",
    "            'push_content': push_content,\n",
    "            'push_ipdatetime': push_ipdatetime})\n",
    "        if push_tag == u'推':\n",
    "            p += 1\n",
    "        elif push_tag == u'噓':\n",
    "            b += 1\n",
    "        else:\n",
    "            n += 1\n",
    "    \n",
    "    # 統計推噓文\n",
    "    # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "    # all 為總共留言數量 \n",
    "    message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "    \n",
    "    # 整理文章資訊\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'article_author': author,\n",
    "        'article_title': title,\n",
    "        'article_date': date,\n",
    "        'article_content': content,\n",
    "        'ip': ip,\n",
    "        'message_count': message_count,\n",
    "        'messages': messages\n",
    "    }\n",
    "    return data\n",
    "\n",
    "import time\n",
    "\n",
    "# 對文章列表送出請求並取得列表主體\n",
    "resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "soup = BeautifulSoup(resp.text)\n",
    "main_list = soup.find('div', class_='bbs-screen')\n",
    "all_data = []\n",
    "all_url = []\n",
    "\n",
    "stime = time.time()\n",
    "# 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "for div in main_list.findChildren('div', recursive=False):\n",
    "    class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "    \n",
    "    # 遇到分隔線要處理的情況\n",
    "    if class_name and 'r-list-sep' in class_name:\n",
    "        print('Reach the last article')\n",
    "        break\n",
    "    \n",
    "    # 遇到目標文章\n",
    "    if class_name and 'r-ent' in class_name:\n",
    "        div_title = div.find('div', class_='title')\n",
    "        a_title = div_title.find('a', href=True)\n",
    "        if a_title:\n",
    "            article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "        else:\n",
    "            article_URL = None\n",
    "            a_title = '<a>本文已刪除</a>'\n",
    "        article_title = a_title.text\n",
    "        print('Parse {} - {}'.format(article_title, article_URL))\n",
    "        \n",
    "        # 把文章連結存在list\n",
    "        if article_URL:\n",
    "            all_url.append(article_URL)\n",
    "\n",
    "# 從這裡丟給子執行緒工作            \n",
    "# 建立 n 個子執行緒，分別去抓文章內容\n",
    "threads = []\n",
    "for i in range(len(all_url)):\n",
    "    threads.append(threading.Thread(target = crawl_article, args = (all_url[i],)))\n",
    "    threads[i].start()\n",
    "\n",
    "# 主執行緒繼續執行自己的工作\n",
    "# ...\n",
    "\n",
    "# 等待所有子執行緒結束\n",
    "for i in range(len(all_url)):\n",
    "    threads[i].join()\n",
    "\n",
    "        \n",
    "etime = time.time()\n",
    "print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 物件導向寫法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTask(threading.Thread):\n",
    "    def __init__(self, task_name):\n",
    "        super(MyTask, self).__init__()\n",
    "        self.task_name = task_name\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Get task: {}\\n\".format(self.task_name))\n",
    "        time.sleep(1)\n",
    "        print(\"Finish task: {}\\n\".format(self.task_name))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = [1,2,3,4,5,6,7,8,9,10]\n",
    "    tasks = []\n",
    "    for i in range(0, 10):\n",
    "        # 建立 task\n",
    "        tasks.append(MyTask(\"task_{}\".format(data[i])))\n",
    "    for t in tasks:\n",
    "        # 開始執行 task\n",
    "        t.start()\n",
    "\n",
    "    for t in tasks:\n",
    "        # 等待 task 執行完畢\n",
    "        # 完畢前會阻塞住主執行緒\n",
    "        t.join()\n",
    "    print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改寫:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [問卦] 有沒有過年氣氛的八卦 - https://www.ptt.cc/bbs/Gossiping/M.1579143778.A.A60.html\n",
      "Parse [新聞] 罷韓風暴 韓國瑜深夜PO文：繼續在高雄打 - https://www.ptt.cc/bbs/Gossiping/M.1579143786.A.E1F.html\n",
      "Parse Re: [問卦] 車陣中賣玉蘭花的若改賣雞排呢? - https://www.ptt.cc/bbs/Gossiping/M.1579143940.A.ADE.html\n",
      "Parse Re: [新聞] 「民進黨不會全力罷韓」　柯文哲：留下韓 - https://www.ptt.cc/bbs/Gossiping/M.1579143956.A.2C7.html\n",
      "Parse [新聞] 竹市鄭宏輝 展開為期一週謝票之旅 - https://www.ptt.cc/bbs/Gossiping/M.1579143986.A.1A0.html\n",
      "Parse [新聞] 蔡英文稱「我們已經是獨立的國家」 耿爽 - https://www.ptt.cc/bbs/Gossiping/M.1579143995.A.763.html\n",
      "Parse Re: [問卦] 小蟲羅德曼有辦法在現代NBA生存嗎? - https://www.ptt.cc/bbs/Gossiping/M.1579144028.A.04C.html\n",
      "Reach the last article\n",
      "共7個連結\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579143778.A.A60.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579143786.A.E1F.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579143940.A.ADE.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579143956.A.2C7.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579143986.A.1A0.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579143995.A.763.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579144028.A.04C.html\n",
      "\n",
      "共用時： 0.7963836193084717\n"
     ]
    }
   ],
   "source": [
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.url = url\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        print(\"Get子執行緒: {}\\n\".format(self.url))\n",
    "\n",
    "        response = requests.get(self.url, cookies={'over18': '1'})\n",
    "\n",
    "        # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "        if response.status_code != 200:\n",
    "            print('Error - {} is not available to access'.format(self.url))\n",
    "            return\n",
    "\n",
    "        # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "        soup = BeautifulSoup(response.text)\n",
    "\n",
    "        # 取得文章內容主體\n",
    "        main_content = soup.find(id='main-content')\n",
    "\n",
    "        # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "        metas = main_content.select('div.article-metaline') #list\n",
    "        author = ''\n",
    "        title = ''\n",
    "        date = ''\n",
    "        if metas:\n",
    "            if metas[0].select('span.article-meta-value')[0]:\n",
    "                author = metas[0].select('span.article-meta-value')[0].string\n",
    "            if metas[1].select('span.article-meta-value')[0]:\n",
    "                title = metas[1].select('span.article-meta-value')[0].string\n",
    "            if metas[2].select('span.article-meta-value')[0]:\n",
    "                date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "            # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "            #\n",
    "            # .extract() 方法可以參考官方文件\n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "            for m in metas:\n",
    "                m.extract()\n",
    "            for m in main_content.select('div.article-metaline-right'):\n",
    "                m.extract()\n",
    "\n",
    "        # 取得留言區主體\n",
    "        pushes = main_content.find_all('div', class_='push')\n",
    "        for p in pushes:\n",
    "            p.extract()\n",
    "\n",
    "        # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "        # 透過 regular expression 取得 IP\n",
    "        # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "        try:\n",
    "            ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "            ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "        except Exception as e:\n",
    "            ip = ''\n",
    "\n",
    "        # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "        # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "        #\n",
    "        # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "        #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "        filtered = []\n",
    "        for v in main_content.stripped_strings:\n",
    "            # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "            if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                filtered.append(v)\n",
    "\n",
    "        # 定義一些特殊符號與全形符號的過濾器\n",
    "        expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "        for i in range(len(filtered)):\n",
    "            filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "        # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "        filtered = [i for i in filtered if i]\n",
    "        content = ' '.join(filtered)\n",
    "\n",
    "        # 處理留言區\n",
    "        # p 計算推文數量\n",
    "        # b 計算噓文數量\n",
    "        # n 計算箭頭數量\n",
    "        p, b, n = 0, 0, 0\n",
    "        messages = []\n",
    "        for push in pushes:\n",
    "            # 假如留言段落沒有 push-tag 就跳過\n",
    "            if not push.find('span', 'push-tag'):\n",
    "                continue\n",
    "\n",
    "            # 過濾額外空白與換行符號\n",
    "            # push_tag 判斷是推文, 箭頭還是噓文\n",
    "            # push_userid 判斷留言的人是誰\n",
    "            # push_content 判斷留言內容\n",
    "            # push_ipdatetime 判斷留言日期時間\n",
    "            push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "            push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "            push_content = push.find('span', 'push-content').strings\n",
    "            push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "            push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "            # 整理打包留言的資訊, 並統計推噓文數量\n",
    "            messages.append({\n",
    "                'push_tag': push_tag,\n",
    "                'push_userid': push_userid,\n",
    "                'push_content': push_content,\n",
    "                'push_ipdatetime': push_ipdatetime})\n",
    "            if push_tag == u'推':\n",
    "                p += 1\n",
    "            elif push_tag == u'噓':\n",
    "                b += 1\n",
    "            else:\n",
    "                n += 1\n",
    "\n",
    "        # 統計推噓文\n",
    "        # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "        # all 為總共留言數量 \n",
    "        message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "        # 整理文章資訊\n",
    "        data = {\n",
    "            'url': self.url,\n",
    "            'article_author': author,\n",
    "            'article_title': title,\n",
    "            'article_date': date,\n",
    "            'article_content': content,\n",
    "            'ip': ip,\n",
    "            'message_count': message_count,\n",
    "            'messages': messages\n",
    "        }\n",
    "        return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    all_url = []\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                all_url.append(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(len(all_url)))\n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(len(all_url)):\n",
    "        threads.append(Crawl_Article(all_url[i]))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用佇列 Queue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [問卦] 為什麼「節儉」會成為被嘲諷的行為？ - https://www.ptt.cc/bbs/Gossiping/M.1579144850.A.F09.html\n",
      "Parse [新聞] 韓國瑜回高雄第四天 出門時間提早了！ - https://www.ptt.cc/bbs/Gossiping/M.1579144888.A.F72.html\n",
      "Parse Re: [新聞] 林宅血案案發最後一通電話首度證實遭監聽 - https://www.ptt.cc/bbs/Gossiping/M.1579144927.A.755.html\n",
      "Parse [問卦] 有烏鴉逆滲透的八卦嗎？告東森新聞！ - https://www.ptt.cc/bbs/Gossiping/M.1579144927.A.AB7.html\n",
      "Parse [問卦] 有沒有強弱懸殊的比賽翻盤瞬間的八卦? - https://www.ptt.cc/bbs/Gossiping/M.1579145066.A.D08.html\n",
      "Parse [問卦] 買哪張演唱會門票最潘? - https://www.ptt.cc/bbs/Gossiping/M.1579145084.A.C06.html\n",
      "Parse [問卦] 要怎麼知道身邊哪些朋友是韓粉 - https://www.ptt.cc/bbs/Gossiping/M.1579145094.A.4EC.html\n",
      "Parse [問卦] 舊鈔都跑到哪裡去了呢? - https://www.ptt.cc/bbs/Gossiping/M.1579145095.A.A5E.html\n",
      "Reach the last article\n",
      "共8個連結\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579144850.A.F09.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579144888.A.F72.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579144927.A.755.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579144927.A.AB7.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579145066.A.D08.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579145084.A.C06.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579145094.A.4EC.html\n",
      "\n",
      "Get子執行緒: https://www.ptt.cc/bbs/Gossiping/M.1579145095.A.A5E.html\n",
      "\n",
      "共用時： 0.7950353622436523\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            return data\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "            article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用lock: \n",
    "被 Lock 的 acquire 與 release 包起來的這段程式碼不會被兩個執行緒同時執行。\n",
    "用來寫入檔案\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Worker(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, num, lock):\n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.num = num\n",
    "        self.lock = lock\n",
    "\n",
    "    def run(self):\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "\n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            print(\"子執行緒 %d 取得lock\" % self.num)\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作\n",
    "            print(\"子執行緒 %d: 寫入檔案 %s\" % (self.num, url))\n",
    "            time.sleep(1)\n",
    "\n",
    "            # 釋放 lock\n",
    "            print(\"子執行緒 %d 釋放lock\" % self.num)\n",
    "            self.lock.release()\n",
    "            \n",
    "#建立一個佇列\n",
    "my_queue = Queue()\n",
    "\n",
    "#假裝放五個URL進去queue\n",
    "for i in range(1,6):\n",
    "    my_queue.put(\"Url %d\" % i)\n",
    "\n",
    "# 建立 lock\n",
    "lock = threading.Lock()\n",
    "\n",
    "#建立2個子執行緒，傳入queue和一個參數和lock\n",
    "my_worker1 = Worker(my_queue, 100, lock)\n",
    "my_worker2 = Worker(my_queue, 200, lock)\n",
    "\n",
    "my_worker1.start()\n",
    "my_worker2.start()\n",
    "\n",
    "my_worker1.join()\n",
    "my_worker2.join()\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改寫：加入能寫入檔案的lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse [問卦] 聽說去斯洛伐克有很多妹妹 - https://www.ptt.cc/bbs/Gossiping/M.1579147723.A.19E.html\n",
      "Parse Re: [新聞] 「卡神」楊蕙如辱外交官案移審 北院完成分案 - https://www.ptt.cc/bbs/Gossiping/M.1579147761.A.EC7.html\n",
      "Parse [問卦] 涮乃葉的本體是咖哩醬ㄇ？ - https://www.ptt.cc/bbs/Gossiping/M.1579147845.A.9FA.html\n",
      "Parse [新聞] 賴品妤高中制服嫩照流出！網暴動：超像日本女星 - https://www.ptt.cc/bbs/Gossiping/M.1579147865.A.A66.html\n",
      "Parse [問卦] 有沒有台灣內社的卦？ - https://www.ptt.cc/bbs/Gossiping/M.1579147873.A.268.html\n",
      "Parse [問卦] 老鷹 - https://www.ptt.cc/bbs/Gossiping/M.1579147873.A.6E6.html\n",
      "Parse Re: [問卦] 抓小雞~(@[email protected])~排妹到底憑什麼可以一直亂嗆?~ - https://www.ptt.cc/bbs/Gossiping/M.1579147891.A.F51.html\n",
      "Parse Re: [問卦]聽說去斯/|||\\ 洛伐克有很多妹妹 - https://www.ptt.cc/bbs/Gossiping/M.1579147892.A.58C.html\n",
      "Parse Re: [新聞] 日媒報導從總統選舉看見日本影響力衰退 - https://www.ptt.cc/bbs/Gossiping/M.1579147919.A.5D4.html\n",
      "Parse [問卦] 美國才是最會畫三角的國家嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1579147949.A.ACA.html\n",
      "Parse [問卦] 美國要台跟中買他的豬牛意義？ - https://www.ptt.cc/bbs/Gossiping/M.1579148033.A.80C.html\n",
      "Parse Re: [問卦] 選完才三天 館長就被背叛了？ - https://www.ptt.cc/bbs/Gossiping/M.1579148040.A.6DE.html\n",
      "Parse [問卦] 有沒有亞太電信的八卦？ - https://www.ptt.cc/bbs/Gossiping/M.1579148057.A.4A5.html\n",
      "Parse [問卦] 小吃攤很少提供番茄醬？？？ - https://www.ptt.cc/bbs/Gossiping/M.1579148085.A.92B.html\n",
      "Parse [問卦] 妹子在外穿拖鞋怎麼了嗎？ - https://www.ptt.cc/bbs/Gossiping/M.1579148113.A.B7B.html\n",
      "Parse Re: [新聞] 日媒報導從總統選舉看見日本影響力衰退 - https://www.ptt.cc/bbs/Gossiping/M.1579148116.A.2BA.html\n",
      "Parse [問卦] 絕對可憐少女有多可憐？ - https://www.ptt.cc/bbs/Gossiping/M.1579148152.A.43D.html\n",
      "Parse [問卦] 在PTT或電視媒體上罵人哪個比較多人看到 - https://www.ptt.cc/bbs/Gossiping/M.1579148167.A.F3F.html\n",
      "Parse [新聞] 台南維冠大樓5黑心建商判賠7.1億　偷工 - https://www.ptt.cc/bbs/Gossiping/M.1579148219.A.8FC.html\n",
      "Parse Re: [新聞] 「數學不到7級分就去學焊接」南韓 - https://www.ptt.cc/bbs/Gossiping/M.1579148329.A.C77.html\n",
      "Reach the last article\n",
      "共20個連結\n",
      "共用時： 1.1736557483673096\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "class Crawl_Article(threading.Thread):\n",
    "    \n",
    "    def __init__(self, queue, lock):\n",
    "        super(Crawl_Article, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.lock = lock\n",
    "\n",
    "    # 原crawl_article，改成子執行緒run任務\n",
    "    def run(self): \n",
    "        # 當 queue 裡面有資料再執行\n",
    "        while self.queue.qsize() > 0:\n",
    "            url = self.queue.get()\n",
    "            #print(\"Get子執行緒: {}\\n\".format(url))\n",
    "\n",
    "            response = requests.get(url, cookies={'over18': '1'})\n",
    "\n",
    "            # 假設網頁回應不是 200 OK 的話, 我們視為傳送請求失敗\n",
    "            if response.status_code != 200:\n",
    "                print('Error - {} is not available to access'.format(url))\n",
    "                return\n",
    "\n",
    "            # 將網頁回應的 HTML 傳入 BeautifulSoup 解析器, 方便我們根據標籤 (tag) 資訊去過濾尋找\n",
    "            soup = BeautifulSoup(response.text)\n",
    "\n",
    "            # 取得文章內容主體\n",
    "            main_content = soup.find(id='main-content')\n",
    "\n",
    "            # 假如文章有屬性資料 (meta), 我們在從屬性的區塊中爬出作者 (author), 文章標題 (title), 發文日期 (date)\n",
    "            metas = main_content.select('div.article-metaline') #list\n",
    "            author = ''\n",
    "            title = ''\n",
    "            date = ''\n",
    "            if metas:\n",
    "                if metas[0].select('span.article-meta-value')[0]:\n",
    "                    author = metas[0].select('span.article-meta-value')[0].string\n",
    "                if metas[1].select('span.article-meta-value')[0]:\n",
    "                    title = metas[1].select('span.article-meta-value')[0].string\n",
    "                if metas[2].select('span.article-meta-value')[0]:\n",
    "                    date = metas[2].select('span.article-meta-value')[0].string\n",
    "\n",
    "                # 從 main_content 中移除 meta 資訊（author, title, date 與其他看板資訊）\n",
    "                #\n",
    "                # .extract() 方法可以參考官方文件\n",
    "                #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n",
    "                for m in metas:\n",
    "                    m.extract()\n",
    "                for m in main_content.select('div.article-metaline-right'):\n",
    "                    m.extract()\n",
    "\n",
    "            # 取得留言區主體\n",
    "            pushes = main_content.find_all('div', class_='push')\n",
    "            for p in pushes:\n",
    "                p.extract()\n",
    "\n",
    "            # 假如文章中有包含「※ 發信站: 批踢踢實業坊(ptt.cc), 來自: xxx.xxx.xxx.xxx」的樣式\n",
    "            # 透過 regular expression 取得 IP\n",
    "            # 因為字串中包含特殊符號跟中文, 這邊建議使用 unicode 的型式 u'...'\n",
    "            try:\n",
    "                ip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "                ip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "            except Exception as e:\n",
    "                ip = ''\n",
    "\n",
    "            # 移除文章主體中 '※ 發信站:', '◆ From:', 空行及多餘空白 (※ = u'\\u203b', ◆ = u'\\u25c6')\n",
    "            # 保留英數字, 中文及中文標點, 網址, 部分特殊符號\n",
    "            #\n",
    "            # 透過 .stripped_strings 的方式可以快速移除多餘空白並取出文字, 可參考官方文件 \n",
    "            #  - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#strings-and-stripped-strings\n",
    "            filtered = []\n",
    "            for v in main_content.stripped_strings:\n",
    "                # 假如字串開頭不是特殊符號或是以 '--' 開頭的, 我們都保留其文字\n",
    "                if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']:\n",
    "                    filtered.append(v)\n",
    "\n",
    "            # 定義一些特殊符號與全形符號的過濾器\n",
    "            expr = re.compile(u'[^一-龥。；，：“”（）、？《》\\s\\w:/-_.?~%()]')\n",
    "            for i in range(len(filtered)):\n",
    "                filtered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "            # 移除空白字串, 組合過濾後的文字即為文章本文 (content)\n",
    "            filtered = [i for i in filtered if i]\n",
    "            content = ' '.join(filtered)\n",
    "\n",
    "            # 處理留言區\n",
    "            # p 計算推文數量\n",
    "            # b 計算噓文數量\n",
    "            # n 計算箭頭數量\n",
    "            p, b, n = 0, 0, 0\n",
    "            messages = []\n",
    "            for push in pushes:\n",
    "                # 假如留言段落沒有 push-tag 就跳過\n",
    "                if not push.find('span', 'push-tag'):\n",
    "                    continue\n",
    "\n",
    "                # 過濾額外空白與換行符號\n",
    "                # push_tag 判斷是推文, 箭頭還是噓文\n",
    "                # push_userid 判斷留言的人是誰\n",
    "                # push_content 判斷留言內容\n",
    "                # push_ipdatetime 判斷留言日期時間\n",
    "                push_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "                push_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "                push_content = push.find('span', 'push-content').strings\n",
    "                push_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')\n",
    "                push_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\n",
    "                # 整理打包留言的資訊, 並統計推噓文數量\n",
    "                messages.append({\n",
    "                    'push_tag': push_tag,\n",
    "                    'push_userid': push_userid,\n",
    "                    'push_content': push_content,\n",
    "                    'push_ipdatetime': push_ipdatetime})\n",
    "                if push_tag == u'推':\n",
    "                    p += 1\n",
    "                elif push_tag == u'噓':\n",
    "                    b += 1\n",
    "                else:\n",
    "                    n += 1\n",
    "\n",
    "            # 統計推噓文\n",
    "            # count 為推噓文相抵看這篇文章推文還是噓文比較多\n",
    "            # all 為總共留言數量 \n",
    "            message_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, 'neutral': n}\n",
    "\n",
    "            # 整理文章資訊\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'article_author': author,\n",
    "                'article_title': title,\n",
    "                'article_date': date,\n",
    "                'article_content': content,\n",
    "                'ip': ip,\n",
    "                'message_count': message_count,\n",
    "                'messages': messages\n",
    "            }\n",
    "            \n",
    "            \n",
    "            # 寫入檔案:單一文章內容\n",
    "            \n",
    "            # 取得 lock\n",
    "            lock.acquire()\n",
    "            #print(\"%s 取得lock\" % url[32:51])\n",
    "\n",
    "            # 不能讓多個執行緒同時進的工作 : 將爬完的資訊存成 json 檔案\n",
    "            #print(\"寫入檔案\")\n",
    "            with open('../Data/PTT_Article.json', 'a+', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "                f.write(\",\")\n",
    "\n",
    "            # 釋放 lock\n",
    "            #print(\"%s 釋放lock\" % url[32:51])\n",
    "            self.lock.release()\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 對文章列表送出請求並取得列表主體\n",
    "    resp = requests.get(PTT_URL, cookies={'over18': '1'})\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    main_list = soup.find('div', class_='bbs-screen')\n",
    "    all_data = []\n",
    "    \n",
    "    Q_url = Queue()\n",
    "\n",
    "    stime = time.time()\n",
    "    # 依序檢查文章列表中的 tag, 遇到分隔線就結束, 忽略這之後的文章\n",
    "    for div in main_list.findChildren('div', recursive=False):\n",
    "        class_name = div.attrs['class']  #['search-bar']['r-ent']['r-ent']...\n",
    "        # 遇到分隔線要處理的情況\n",
    "        if class_name and 'r-list-sep' in class_name:\n",
    "            print('Reach the last article')\n",
    "            break\n",
    "        # 遇到目標文章\n",
    "        if class_name and 'r-ent' in class_name:\n",
    "            div_title = div.find('div', class_='title')\n",
    "            a_title = div_title.find('a', href=True)\n",
    "            \n",
    "            if a_title:\n",
    "                article_URL = urljoin(PTT_URL, a_title['href'])\n",
    "                article_title = a_title.text\n",
    "            else:\n",
    "                article_URL = None\n",
    "                a_title = '<a>本文已刪除</a>'\n",
    "                article_title = a_title\n",
    "                \n",
    "            #article_title = a_title.text\n",
    "            print('Parse {} - {}'.format(article_title, article_URL))\n",
    "            # 把文章連結存在list\n",
    "            if article_URL:\n",
    "                Q_url.put(article_URL)\n",
    "    \n",
    "    print('共{}個連結'.format(Q_url.qsize()))\n",
    "    \n",
    "    # 建立 lock\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    # 從這裡丟給子執行緒工作            \n",
    "    # 建立 n 個子執行緒，分別去抓文章內容\n",
    "    threads = []\n",
    "    for i in range(Q_url.qsize()):\n",
    "        threads.append(Crawl_Article(Q_url, lock))\n",
    "        threads[i].start()\n",
    "\n",
    "    # 主執行緒繼續執行自己的工作\n",
    "    # ...\n",
    "\n",
    "    # 等待所有子執行緒結束\n",
    "    for i in range(len(all_url)):\n",
    "        threads[i].join()\n",
    "\n",
    "\n",
    "    etime = time.time()\n",
    "    print('共用時：',etime-stime )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
